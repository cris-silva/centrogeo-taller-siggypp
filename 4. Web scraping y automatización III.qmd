---
title: "4. Web scraping y automatización III"
format: html
editor: visual
---

## Introducción

Existen técnicas que nos permiten automatizar los procesos necesarios para extraer y procesar información de diversas fuentes, siendo una de las principales Internet. Aunque en ocasiones los datos que necesitamos están disponibles en páginas web de acceso abierto, el formato en que los consultamos no siempre es el más apropiado o sencillo de manejar.

En esta sesión, de la misma manera que en la anterior, se utilizarán dos técnicas para automatizar la extracción y geocodificación de datos provenientes de Internet. La primera técnica es el raspado web (*web scraping*), que permite extraer contenido de una página web programando el explorador para controlar y modificar los parámetros utilizados por las páginas de consulta. Esto nos permite obtener información automáticamente, evitando el trabajo repetitivo que requeriría hacerlo manualmente. La segunda técnica es la geocodificación automática, que consiste en obtener las coordenadas de una lista de direcciones de manera automatizada mediante un servicio web.

Es importante utilizar estas técnicas con discreción, ya que tienen implicaciones éticas y ciertos márgenes de imprecisión. Sin embargo, son herramientas muy útiles para adquirir datos difíciles de obtener por medios convencionales.

## Objetivo

El objetivo de esta práctica es obtener una base de datos geográfica de inmuebles históricos en el país. La información oficial puede ser obtenida desde el sitio web del Catálogo Nacional de Monumentos Históricos Inmuebles del INAH ubicado en <https://catalogonacionalmhi.inah.gob.mx/>. Los datos son públicos, sin embargo, no existe ningún medio para poder descargarla en algún formato apropiado para construir la base de datos y sólo es posible hacer consultas de manera manual para obtener los datos en forma de fichas individuales.

En el catálogo existen más de 98 mil registros y la descarga manual de los datos sería extremadamente laboriosa y requeriría mucho tiempo y trabajo para lograr recopilar todos los datos, por esta razón es que es necesario utilizar técnicas de automatización y *web scraping* para poder construir la base de datos deseada de una manera más sencilla y eficiente.

Durante el transcurso de esta sesión, usarás Selenium y las funciones del paquete `RSelenium` para controlar el explorador Firefox y automatizar la consulta de información de múltiples registros de inmuebles en la página del INAH, y en combinación con el paquete `rvest` extraerás los datos obtenidos de cada consulta para acumularlos en una misma tabla resultante, la cual geocodificarás de manera automática usando el paquete `tidygeocoder` y los servicios de Here Maps para obtener las coordenadas a partir de las direcciones extraídas y finalmente, construirás una capa geográfica con la que se puedan realizar análisis geoespaciales usando el paquete `sf`.

## ¿Cómo funciona la página web de consulta?

Haz una revisión del sitio con el que trabajarás en esta práctica para conocerlo. Abre en tu explorador la página principal del Catálogo Nacional de Monumentos Históricos Inmuebles del INAH (<https://catalogonacionalmhi.inah.gob.mx/>), y observa que tiene un botón *Consulta Pública* que permite hacer la consulta de información:

![Página de inicio del sitio](Imagenes/InicioCatalogo.png){fig-alt="Página de inicio del sitio" fig-align="center" width="468"}

Al hacer clic en este botón el sitio te redirigirá a otra página donde puedes hacer búsquedas de información:

![Página de consulta pública](Imagenes/PaginaConsulta.png){fig-alt="Página de consulta pública" fig-align="center" width="468"}

Si haces clic en el botón *Búsqueda avanzada* aparecerán más opciones para incluir criterios de búsqueda:

![Opciones de búsqueda avanzada](Imagenes/BusquedaAvanzada.png){fig-alt="Opciones de búsqueda avanzada" fig-align="center" width="449"}

Haz una consulta al azar, selecciona al menos una *Entidad Federativa* y un *Municipio / Alcaldía* y presiona el botón *Buscar* para obtener los registros de los inmuebles correspondientes:

![Resultados de la consulta](Imagenes/ResultadosConsulta.png){fig-alt="Resultados de la consulta" fig-align="center" width="400"}

Da clic en la liga *Ver detalle* de cualquiera de los resultados. Se abrirá la página de la ficha correspondiente a ese inmueble, y encontrarás varias secciones de información, como *tipo de ficha, localización, identificación,* etc. que están contraídas. Presiona el botón *Expandir todos* para que los paneles de todas las secciones se abran y muestren toda la información disponible sobre el inmueble:

![Ejemplo de una ficha de datos de inmueble.](Imagenes/ConsultaPublica.png){alt="Ejemplo de una ficha de datos de inmueble." fig-alt="Ejemplo de una ficha de datos de inmueble." fig-align="center" width="30%"}

Ahora puedes ver en esta ficha todos los datos que deberás extraer. En la barra de direcciones de tu explorador, observa la dirección de la página web de la ficha de datos, debe ser un URL similar a este:

<https://catalogonacionalmhi.inah.gob.mx/consulta_publica/detalle/82800>

Normalmente, los sitios web de consulta se conectan a una base de datos para obtener los datos de cada registro, y utilizan un identificador único para recuperar la información solicitada. El número al final del URL anterior corresponde al identificador único de la ficha. Sustituye este número e intenta abrir esa dirección en el explorador, por ejemplo:

[https://](https://catalogonacionalmhi.inah.gob.mx/consulta_publica/detalle/82800)[catalogonacionalmhi](https://catalogonacionalmhi.inah.gob.mx/consulta_publica/detalle/9000)[.inah.gob.mx/consulta_publica/detalle/9000](https://catalogonacionalmhi.inah.gob.mx/consulta_publica/detalle/82800)

Se abrirá una ficha con los datos correspondientes a otro inmueble. Esto quiere decir que se puede modificar la dirección cambiando únicamente este identificador para obtener cada una de las fichas de todo el catálogo.

Si agregas al URL un número de identificador que no existe en el catálogo, te enviará de vuelta a la página de inicio del sitio web. Mediante prueba y error, puedes determinar cuántas fichas hay en total en el catálogo; en este caso puedes determinar que existen 98,703 registros (qué es el número de identificador máximo que puedes usar sin que la URL te mande a la página inicial). En esta práctica prepararás el mecanismo de extracción de datos para cada una de estas fichas.

Por otro lado, existen fichas que tienen diferentes datos dependiendo del tipo de inmueble. Estos se distinguen usando colores en el cintillo del número de captura.

![Tipos de fichas](Imagenes/TiposFicha.png){fig-alt="Tipos de fichas" fig-align="center" width="532"}

Será necesario asegurar que todos estos datos se guarden de manera ordenada en la base final, ya que existirán columnas con datos para algunos registros que pueden no existir en otros y viceversa. Esto se detalla más adelante en la práctica.

## Metodología

Antes de comenzar, es conveniente definir los pasos a seguir para realizar el procedimiento de extracción de datos en esta práctica:

1.  Observar cómo funciona la página web de consulta.
2.  Identificar la estructura de la página de consulta y los elementos que se van a extraer.
3.  Los datos en la página están enmarcados en paneles de diferentes secciones de información, como datos generales, tipo de ficha, localización, identificación, etc. Será conveniente crear procedimientos para extraer los datos de cada sección de la página de consulta.
4.  Programar las instrucciones necesarias para extraer (raspar) todos los datos de una ficha individual, como ejemplo.
5.  Una vez programado el método de raspado para una ficha de ejemplo, se unen todas las instrucciones en una sola función que nos permita repetir el procedimiento de extracción de datos para todas fichas del catálogo.
6.  Iterar el proceso para cada una de las 98,703 fichas de registro de inmuebles del sitio.
7.  Acumular cada uno de los registros extraídos en una misma tabla, ordenarla y limpiarla.
8.  Geocodificar las direcciones usando el servicio web de Here.
9.  Convertir la tabla resultante de la geocodificación en una capa geográfica de inmuebles de tipo simple features (sf)*.*
10. Visualizar una muestra aleatoria de los puntos de la capa de inmuebles geocodificada en un mapa interactivo y comprobar visualmente la precisión.
11. Finalmente, guardar la capa de inmuebles en un archivo geopackage (.gpkg) como capa geográfica.

## Preparación del entorno

Ejecuta el siguiente bloque para cargar los paquetes que usarás en esta sesión de trabajo:

```{r Cargar los paquetes}
library(tidyverse)
library(janitor)
library(rvest)
library(RSelenium)
library(tidygeocoder)
library(sf)
library(tmap)
```

### Iniciar servidor de Selenium

Utiliza la función `rsDriver` del paquete `RSelenium` para iniciar un servidor de Selenium y guarda el resultado en la variable `servidor_selenium`, indicando que vas a usar el explorador Firefox con el parámetro `browser = "firefox"` y omite la carga de los demás exploradores usando los parámetros `chromever = NULL, iedrver = NULL, phantomver = NULL`:

```{r Iniciar Selenium}
servidor_selenium <- rsDriver(browser = "firefox",
                              chromever = NULL,
                              iedrver = NULL,
                              phantomver = NULL)
```

El explorador Firefox se ha iniciado y en la barra de direcciones a la izquierda encontrarás un icono de robot. Esto indica que Selenium ya está conectado a Firefox y está listo para enviarle instrucciones. Ahora, crea un controlador para Selenium usando el atributo `client` de `servidor_selenium` y asígnalo a una instancia que se llame `controlador`:

```{r Crear el controlador del explorador web}
controlador <- servidor_selenium$client
```

## Procesamiento de una ficha de ejemplo

### Abrir la página de la ficha

Ahora que ya tienes el controlador, envía a Selenium la instrucción de abrir la página de detalle de la ficha agregando al URL un identificador único de prueba:

1.  Asigna a la variable `pagina_fecha` la dirección base de la página web de la ficha (`"https://catalogonacionalmhi.inah.gob.mx/consulta_publica/detalle/"`).
2.  Asigna a la variable `id_ficha` un valor de prueba, en este caso `9000`
3.  Usa el método `controlador$navigate` para abrir la URL obtenida de concatenar `pagina_ficha` e `id_ficha` usando la función `str_c`.

```{r Abrir la página de descarga}
# Define la URL base de la página de la ficha:
pagina_ficha <- "https://catalogonacionalmhi.inah.gob.mx/consulta_publica/detalle/"

# Identificador único de prueba:
id_ficha <- 9000 

# Abrir la página web con la dirección y el identificador combinados:
controlador$navigate(str_c(pagina_ficha, id_ficha))
```

### Identificación de los controles de la página

Usando el inspector web en la página de la ficha, determina la dirección *xPath* del botón *Expandir todos*, en este caso es `//*[@id="btnExpandir"]`.

Ahora, crea un control para el botón que se llame `boton_expandir` usando la función `controlador$findElement` con los parámetros `using = "xpath"` y `value = '//*[@id="btnExpandir"]'`. Usa el control `boton_expandir_todos$clickElement()` para expandir todos los contenedores de la ficha y visualizar todos los datos:

```{r Expandir contenedores}
# Crear una instancia del botón Expandir todos:
boton_expandir_todos <- controlador$findElement(using = "xpath",
                                                value = '//*[@id="btnExpandir"]')

# Presionar el botón para expandir los contenedores
boton_expandir_todos$clickElement()
```

Guarda una copia de la página (su código fuente) en la variable `codigo_fuente` usando el método del controlador del explorador `controlador$getPageSource()` y usa la función `read_html` de `rvest` para leer la página copiada en `codigo_fuente[[1]]` (el primer elemento de la lista es el texto del código fuente), guarda el resultado de la lectura en la variable `html_ficha` de donde rasparás los datos:

```{r Guardar el código fuente de la ficha}
# Guarda el contenido de la página en una variable
codigo_fuente <- controlador$getPageSource()

# Leer con rvest el código fuente de la lista obtenida con Selenium:
html_ficha <- 
  read_html(codigo_fuente[[1]])
```

### Extracción de los datos de cada sección

Para obtener los datos de la ficha, será necesario considerar lo siguiente:

1.  En la ficha, los datos se presentan usando el nombre de la variable, dos puntos y el valor correspondiente (por ejemplo `Nombre: Sepulcro de Manuel Escudero y Verdugo`).
2.  Los datos se deberán extraer sección por sección como vector de texto con la convención anterior, si hay datos que no tienen un nombre de variable con `:` será necesario agregarlos.
3.  De acuerdo con lo anterior, es posible construir una tabla con todos las variables y valores si todos tienen esta misma estructura y al final se separa en dos columnas usando `:` como separador.
4.  Se obtendrá una tabla con dos columnas, la primera contendrá el nombre de las variables y la segunda sus valores correspondientes.
5.  Será necesario transformar esta tabla de valores y variables para que las variables estén como columnas los valores representen un registro de fila. Para ello, será necesario hacer un *pivote,* es decir, pasar filas a columnas.
6.  Una vez transpuestos, los datos se acumularán en

Observa y ejecuta el siguiente bloque para realizar la manipulación de datos que te permitirá extraer los datos del panel de la primera sección de la ficha:

1.  A partir de tu copia de la página en `html_ficha` busca el primer panel (con dirección *xPath* `//*[@id="encabezado"]`), usa la función `html_children` cuatro veces para identificar los elementos de texto (datos) que están en el cuarto nivel de la estructura web, extrae el texto sin HTML usando la función `html_text` y limpia el resultado eliminando saltos de página innecesarios con `str_remove_all("\n")` y los espacios en blanco antes y después de cada texto con `str_squish.`
2.  

> Recuerda que puedes ejecutar parcialmente el código para comprobar sus resultados poco a poco, si seleccionas un fragmento de código y presionas Ctrl+Enter.

```{r Extraer datos del encabezado}
# Buscar los elementos donde están los datos, extraer y limpiar el texto
datos_contenedor_encabezado <-
  html_ficha %>% 
  html_element(xpath = '//*[@id="encabezado"]') %>% 
  html_children() %>% 
  html_children() %>% 
  html_children() %>% 
  html_children() %>% # Expandir hasta los elementos de cuarto nivel (filas)
  html_text() %>% 
  str_remove_all("\n") %>% # Eliminar todos los saltos de línea
  str_squish() # Eliminar los espacios en blanco antes y después del texto

# Agregar nombres de variables faltantes
datos_contenedor_encabezado[1] <- 
  str_c("Tipo: ", datos_contenedor_encabezado[1])

datos_contenedor_encabezado[5] <- 
  str_c("Validación: ", datos_contenedor_encabezado[5])

# Cosnervar las 5 primeras filas
datos_contenedor_encabezado <-
  datos_contenedor_encabezado[1:5]

# Ver el resultado
datos_contenedor_encabezado
```

```{r Extraer tipo de ficha}
# Buscar el siguiente contenedor y extraer los datos
datos_contenedor_1 <-
  html_ficha %>% 
  html_element(xpath = '//*[@id="divSeccion1"]') %>% # Buscar el elemento del título
  html_children() %>% 
  html_children() %>% 
  html_children() %>% # Expandir hasta los elementos de tercer nivel (filas)
  html_text() %>% # Extraer el texto de los elementos
  str_remove_all("\n") %>% # Eliminar todos los saltos de línea
  str_squish() # Eliminar los espacios en blanco antes y después del texto

# Agregar un prefijo con la sección
datos_contenedor_1 <- str_c("S1_", datos_contenedor_1)

# Ver el resultado
datos_contenedor_1
```

Extraer la URL del mapa de Google con las coordenadas:

```{r Extraer URL del mapa}
# Buscar la URL origen del mapa de Google
datos_contenedor_mapa <-
  html_ficha %>%
  html_element(xpath = '//*[@id="divSeccion2"]') %>% # Buscar la sección donde está el mapa
  html_element("a") %>% # Buscar el primer link con la URL del mapa
  html_attr("href") %>%  # Extraer la URL del mapa
  replace_na("") # Si no hay mapa, reemplazar el valor vacío NA con un espacio

# Agregar el nombre del campo al valor:
datos_contenedor_mapa <-
  str_c("S2_Google Maps: ",
        datos_contenedor_mapa)

# Ver el resultado
datos_contenedor_mapa
```

```{r Extraer datos de Localización}
# Buscar el siguiente contenedor y extraer los datos
datos_contenedor_2 <-
  html_ficha %>% 
  html_element(xpath = '//*[@id="divSeccion2"]') %>% # Buscar el contenedor con los datos
  html_children() %>% 
  html_children() %>% # Bajar al segundo nivel (filas)
  html_text() %>% # Extraer el texto de los elementos
  str_remove_all("\n") %>% # Eliminar todos los saltos de línea
  str_squish() # Eliminar los espacios en blanco antes y después del texto

# Eliminar las dos primeras filas que contienen textos de los controles del mapa
datos_contenedor_2 <-
  datos_contenedor_2[c(-1, -2)]

# Eliminar la fila de título de "Otra localización:"
datos_contenedor_2 <-
  datos_contenedor_2[datos_contenedor_2 != "Otra localización:"]

# Agregar el nombre de la variable al dato de "Otra localización:"
datos_contenedor_2[length(datos_contenedor_2)] <-
  str_c("Otra localización: ", datos_contenedor_2[length(datos_contenedor_2)])

# Agregar un prefijo con la sección
datos_contenedor_2 <-
  str_c("S2_", datos_contenedor_2)

# Ver el resultado
datos_contenedor_2
```

```{r Extraer datos de Identificación}
# Buscar el siguiente contenedor y extraer los datos
datos_contenedor_3_original <-
  html_ficha %>% 
  html_element(xpath = '//*[@id="divSeccion3"]') %>% # Buscar el siguiente contenedor
  html_children() %>% 
  html_children() %>% # Bajar al segundo nivel (filas)
  html_text() %>%  # Extraer el texto de cada elemento que se encontró
  str_remove_all("\n") %>% # Eliminar todos los saltos de línea
  str_squish() # Eliminar los espacios en blanco antes y después del texto

# Separar los datos que vienen juntos en una misma fila
datos_contenedor_3_separados <-
  datos_contenedor_3_original[10] %>% 
  str_replace("Género:", "|Género:") %>% # Agregar un separador
  str_replace("Tipo Arquitectónico:", "|Tipo Arquitectónico:") %>% # Agregar un separador
  str_split("\\|") %>% # Separar usando el caracter definido
  unlist() # Convertir la lista resultante en un vector de tipo caracter

# Eliminar las filas de encabezados, agregar los datos separados y conservar los datos con nombre
datos_contenedor_3 <- c(
  datos_contenedor_3_original[c(1, 3:5, 7:8)],
  datos_contenedor_3_separados,
  datos_contenedor_3_original[12]
)

# Agregar un prefijo para diferenciar los datos originales de los actuales
datos_contenedor_3[2:4] <-
  str_c("original_", datos_contenedor_3[2:4])

datos_contenedor_3[7:9] <-
  str_c("actual_", datos_contenedor_3[7:9])

# Agregar un prefijo con la sección
datos_contenedor_3 <-
  str_c("S3_", datos_contenedor_3)

# Ver el resultado
datos_contenedor_3
```

```{r Extraer datos de Aspectos Legales}
# Buscar el siguiente contenedor y extraer los datos
datos_contenedor_4_original <-
  html_ficha %>% 
  html_element(xpath = '//*[@id="divSeccion4"]') %>% # Buscar el siguiente contenedor
  html_children() %>%
  html_children() %>%
  html_children() %>% # Bajar al tercer nivel (filas)
  html_text() %>% # Extraer el texto de cada elemento que se encontró
  str_remove_all("\n") %>% # Eliminar todos los saltos de línea
  str_squish() # Eliminar los espacios en blanco antes y después del texto

# Separar los datos que vienen juntos en una misma fila
datos_contenedor_4_separados <-
  datos_contenedor_4_original[5] %>% 
  str_replace("Ubicado en la Zona de Monumentos Históricos:", "|Ubicado en la Zona de Monumentos Históricos:") %>% 
  str_replace("Sitio Inscrito en la lista de patrimonio Mundial UNESCO:", "|Sitio Inscrito en la lista de patrimonio Mundial UNESCO:") %>% 
  str_split("\\|") %>% 
  unlist()

# Unir los datos extraídos correctamente y los que se separaron
datos_contenedor_4 <-
  c(
    datos_contenedor_4_original[1:4],
    datos_contenedor_4_separados
  )

# Agregar un prefijo con la sección
datos_contenedor_4 <-
  str_c("S4_", datos_contenedor_4)

# Ver el resultado
datos_contenedor_4
```

```{r Extraer datos de Información Histórica}
# Buscar el siguiente contenedor y extraer los datos
datos_contenedor_5 <-
  html_ficha %>% 
  html_element(xpath = '//*[@id="divSeccion6"]') %>%  # Buscar el siguiente contenedor
  html_children() %>% 
  html_children() %>% 
  html_children() %>% # Bajar al tercer nivel (filas)
  html_text() %>%  # Extraer el texto de cada elemento que se encontró
  str_remove_all("\n") %>% # Eliminar todos los saltos de línea
  str_squish() # Eliminar los espacios en blanco antes y después del texto

# Agregar los nombres de las variables a los datos:
datos_contenedor_5[5] <- 
  str_c(datos_contenedor_5[4], " ", datos_contenedor_5[5])

datos_contenedor_5[8] <- 
  str_c(datos_contenedor_5[7], " ", datos_contenedor_5[8])

datos_contenedor_5[11] <- 
  str_c(datos_contenedor_5[10], " ", datos_contenedor_5[11])

# Quitar las filas de encabezados 
datos_contenedor_5 <-
  datos_contenedor_5[c(-1, -4, -6, -7, -10)]

# Agregar un prefijo con la sección
datos_contenedor_5 <-
  str_c("S5_", datos_contenedor_5)
  
# Ver el resultado
datos_contenedor_5
```

```{r Extraer datos de Fuentes Consultadas}
# Buscar el siguiente contenedor y extraer los datos
datos_contenedor_6 <-
  html_ficha %>% 
  html_element(xpath = '//*[@id="divSeccion7"]') %>% # Buscar el siguiente contenedor
  html_children() %>% # Bajar al primer nivel (filas)
  html_text() %>% # Extraer el texto de cada elemento que se encontró
  str_remove_all("\n") %>% # Eliminar todos los saltos de línea
  str_squish() # Eliminar los espacios en blanco antes y después del texto

# Quitar filas vacías
datos_contenedor_6 <-
  datos_contenedor_6[datos_contenedor_6 != ""]

# Agregar un prefijo con la sección
datos_contenedor_6 <-
  str_c("S6_", datos_contenedor_6)

# Ver el resultado
datos_contenedor_6
```

```{r Extraer datos de monografía}
# Buscar el siguiente contenedor y extraer los datos
datos_contenedor_7 <-
  html_ficha %>% 
  html_element(xpath = '//*[@id="divSeccion8"]') %>% # Buscar el siguiente contenedor
  html_children() %>% # Bajar al primer nivel (filas)
  html_text() %>% # Extraer el texto de cada elemento que se encontró
  str_remove_all("\n") %>% # Eliminar todos los saltos de línea
  str_squish() # Eliminar los espacios en blanco antes y después del texto

# Quitar filas vacías
datos_contenedor_7 <-
  datos_contenedor_7[datos_contenedor_7 != ""]

# Agregar un prefijo con la sección y el nombre de la variable
datos_contenedor_7 <-
  str_c("S7_Monografia: ", datos_contenedor_7)

# Ver el resultado
datos_contenedor_7
```

```{r Extraer datos de Descripción Arquitectónica}
# Buscar el siguiente contenedor y extraer los datos
datos_contenedor_8 <-
  html_ficha %>% 
  html_element(xpath = '//*[@id="divSeccion11"]') %>% # Buscar el siguiente contenedor
  html_children() %>% # Bajar al primer nivel (filas)
  html_text() %>% # Extraer el texto de cada elemento que se encontró
  str_remove_all("\n") %>% # Eliminar todos los saltos de línea
  str_squish() # Eliminar los espacios en blanco antes y después del texto

# Quitar filas vacías
datos_contenedor_8 <-
  datos_contenedor_8[datos_contenedor_8 != ""]

datos_contenedor_8 <-
  str_c("S8_", datos_contenedor_8)

# Ver el resultado
datos_contenedor_8
```

```{r Extraer Características}
# Buscar el siguiente contenedor y extraer los datos
datos_contenedor_9 <-
  html_ficha %>% 
  html_element(xpath = '//*[@id="divSeccion12"]') %>% # Buscar el siguiente contenedor
  html_children() %>% 
  html_children() %>% 
  html_children() %>% # Bajar al tercer nivel (filas)
  html_text() %>% # Extraer el texto de cada elemento que se encontró
  str_remove_all("\n") %>% # Eliminar todos los saltos de línea
  str_squish() # Eliminar los espacios en blanco antes y después del texto

# Definir un prefijo inicial
prefijo <- ""

# Agregar el prefijo de encabezado correspondiente a cada elemento extraido
for(i in 1:length(datos_contenedor_9)) {
  
  if(!str_detect(datos_contenedor_9[i], "^*:")) {
    prefijo <- datos_contenedor_9[i]
  }
  
  datos_contenedor_9[i] <- str_c(str_trunc(prefijo, 6, ellipsis = ""), # Usar las primeras letras del encabezado
                                 "_", 
                                 datos_contenedor_9[i])
  
}

# Agregar un prefijo con la sección
datos_contenedor_9 <- str_c("S9_", datos_contenedor_9) 

# Quitar las cabeceras (elementos sin ":")
datos_contenedor_9 <- 
  datos_contenedor_9[str_detect(datos_contenedor_9, "^*:")]

# Ver el resultado
datos_contenedor_9
```

```{r Extraer Fecha de Elaboración o Actualización}
# Buscar el siguiente contenedor y extraer los datos
datos_contenedor_10 <-
  html_ficha %>% 
  html_element(xpath = '//*[@id="divSeccion17"]') %>% # Buscar el siguiente contenedor
  html_children() %>% 
  html_text() %>% # Extraer el texto de cada elemento que se encontró
  str_remove_all("\n") %>% # Eliminar todos los saltos de línea
  str_squish() # Eliminar los espacios en blanco antes y después del texto

# Agregar el prefijo y conservar la fecha más reciente:
datos_contenedor_10 <- 
  str_c("S10_", datos_contenedor_10) %>% 
  head()

# Ver el resultado
datos_contenedor_10
```

```{r Extrae datos PAHJE}
# Buscar el siguiente contenedor y extraer los datos
datos_contenedor_11 <-
  html_ficha %>% 
  html_element(xpath = '//*[@id="divSeccion22"]') %>% # Buscar el siguiente contenedor
  html_element(xpath = '//*[@id="divSeccion22"]/div[1]/div[2]/p') %>% 
  html_text() %>% # Extraer el texto de cada elemento que se encontró
  str_remove_all("\n") %>% # Eliminar todos los saltos de línea
  str_squish() # Eliminar los espacios en blanco antes y después del texto
  
# Agregar el nombre del campo al valor:
datos_contenedor_11 <- c(str_c("S11_Planoteca del AHJE: ", datos_contenedor_11),
                         str_c("S11_URL Planoteca del AHJE: https://catalogonacionalmhi.inah.gob.mx/consulta_publica/generarPdfPlanoteca/", 9000))

# Ver el resultado
datos_contenedor_11
```

```{r Extraer datos AHJE}
# Buscar el siguiente contenedor y extraer los datos
datos_contenedor_12 <-
  html_ficha %>% 
  html_element(xpath = '//*[@id="divSeccion23"]') %>% # Buscar el siguiente contenedor
  html_element(xpath = '//*[@id="divSeccion23"]/div[1]/div[2]') %>% 
  html_text() %>% # Extraer el texto de cada elemento que se encontró
  str_remove_all("\n") %>% # Eliminar todos los saltos de línea
  str_squish() # Eliminar los espacios en blanco antes y después del texto

# Agregar el nombre del campo al valor:
datos_contenedor_12 <- c(str_c("S12_AHJE: ", datos_contenedor_12),
                         str_c("S12_URL AHJE: https://catalogonacionalmhi.inah.gob.mx/consulta_publica/generarPdfArchivoHistorico/", 9000))

# Ver el resultado
datos_contenedor_12
```

```{r Extraer datos FCRV}
# Buscar el siguiente contenedor y extraer los datos
datos_contenedor_13 <-
  html_ficha %>% 
  html_element(xpath = '//*[@id="divSeccion24"]') %>% # Buscar el siguiente contenedor
  html_element(xpath = '//*[@id="divSeccion24"]/div[1]/div[2]') %>% 
  html_text() %>% # Extraer el texto de cada elemento que se encontró
  str_remove_all("\n") %>% # Eliminar todos los saltos de línea
  str_squish() # Eliminar los espacios en blanco antes y después del texto

# Agregar el nombre del campo al valor:
datos_contenedor_13 <- c(str_c("S13_FCRV: ", datos_contenedor_13),
                         str_c("S13_URL FCRV: https://catalogonacionalmhi.inah.gob.mx/consulta_publica/generarPdfFototeca/", 9000))

# Ver el resultado
datos_contenedor_13
```

Unir los datos extraídos:

```{r Unir los valores extraidos}
# Unir los valores extraidos
datos_contenedor <- c(str_c("ID: ", 9000), # Agregar el ID de la página
                      datos_contenedor_1,
                      datos_contenedor_mapa,
                      datos_contenedor_2,
                      datos_contenedor_3,
                      datos_contenedor_4,
                      datos_contenedor_5,
                      datos_contenedor_6,
                      datos_contenedor_7,
                      datos_contenedor_8,
                      datos_contenedor_9,
                      datos_contenedor_10,
                      datos_contenedor_11,
                      datos_contenedor_12,
                      datos_contenedor_13)

datos_contenedor
```

Convertir el listado de datos extraídos en un registro de tabla:

Para que `pivot_wider` funcione correctamente en este caso, es necesario que no haya nombres de campos repetidos.

```{r Contruir el registro}
# Construir un tibble con un registro con los datos extraídos
registro_extraido <-
  tibble(lista = datos_contenedor) %>% # Nuevo tibble con una columna con la lista
  separate(lista, into = c("variable", "valor"), sep = ":", extra = "merge") %>% # Separar en los dos puntos
  mutate(variable = make_clean_names(variable)) %>%  
  pivot_wider(names_from = variable, values_from = valor) %>% # Voltear de filas a columnas
  glimpse() # Ver la estructura de datos
```

NOTA: NO HICE UNA FUNCIÓN POR QUE CADA DIV ESTÁ CONSTRUIDO DIFERENTE

## Automatización y extracción masiva

Hasta este punto, ya desarrollaste la secuencia de operaciones que se requieren para descargar los datos de una página en particular. Ahora debes repetir este proceso para cambiando el ID de la dirección de la página que genera la ficha para obtener los datos de todas las fichas posibles:

```         
https://catalogonacionalmhi.inah.gob.mx/consulta_publica/detalle/[ID de la página]
```

Donde debes iterar el valor de \[`ID de la página]` con números enteros.

Hasta este momento, es conveniente cerrar el servidor de Selemium y borrar el entorno para comenzar con la automatización de la página para la extración masiva de los datos.

Cierra las ventanas abiertas del explorador y apaga el servidor de Selenium:

```{r Cerrar explorador y servidor de Selenium}
# Cerrar todas las ventanas abiertas
controlador$closeall()

# Apagar el servidor de Selenium
servidor_selenium$server$stop()

# Borra todas las variables y objetos del entorno
rm(list = ls())
```

### Función para la extracción de datos

A partir de la secuencia de operaciones que creaste anteriormente, junta las instrucciones anteriores y construye una función que permita hacer el *scraping*, definiendo como parámetros el ID de la URL de la página de consulta y el controlador de Selenium que usarás para controlar el cambio de páginas.

```{r Función para extracción de datos}
# Definir la función para extraer los datos de una ficha
extraer_datos <- function(id_pagina, controlador, espera_carga = 3, espera_expandir = 3) {
  
  # Construir la URL usando el ID de la página que se pasa como parámetro (id_pagina):
  pagina_ficha <- str_c("https://catalogonacionalmhi.inah.gob.mx/consulta_publica/detalle/", id_pagina)
  
  # Ordenar al explorador que abra la página de la ficha con el ID deseado
  controlador$navigate(pagina_ficha)
  
  # Esperar a que cargue la página
  Sys.sleep(espera_carga)
  
  # Crear una instancia del botón Expandir todos
  boton_expandir_todos <- controlador$findElement(using = "xpath",
                                                  value = '//*[@id="btnExpandir"]')
  
  # Presionar el botón para expandir los contenedores
  boton_expandir_todos$clickElement()
  
  # Esperar a que se expandan las secciones
  Sys.sleep(espera_expandir)
  
  # Guarda el contenido de la página en una variable
  codigo_fuente <- controlador$getPageSource()
  
  # Extraer con rvest el código fuente de la lista obtenida con Selenium:
  html_ficha <- 
    read_html(codigo_fuente[[1]])
  
  
  # Buscar los elementos donde están los datos, extraer y limpiar el texto
  datos_contenedor_encabezado <-
    html_ficha %>% 
    html_element(xpath = '//*[@id="encabezado"]') %>% 
    html_children() %>% 
    html_children() %>% 
    html_children() %>% 
    html_children() %>% # Expandir hasta los elementos de cuarto nivel (filas)
    html_text() %>% 
    str_remove_all("\n") %>% # Eliminar todos los saltos de línea
    str_squish() # Eliminar los espacios en blanco antes y después del texto
  
  # Agregar nombres de variables faltantes
  datos_contenedor_encabezado[1] <- 
    str_c("Tipo: ", datos_contenedor_encabezado[1])
  
  datos_contenedor_encabezado[5] <- 
    str_c("Validación: ", datos_contenedor_encabezado[5])
  
  # Cosnervar las 5 primeras filas
  datos_contenedor_encabezado <-
    datos_contenedor_encabezado[1:5] 
  
  
  # Buscar el siguiente contenedor y extraer los datos
  datos_contenedor_1 <-
    html_ficha %>% 
    html_element(xpath = '//*[@id="divSeccion1"]') %>% # Buscar el elemento del título
    html_children() %>% 
    html_children() %>% 
    html_children() %>% # Expandir hasta los elementos de tercer nivel (filas)
    html_text() %>% # Extraer el texto de los elementos
    str_remove_all("\n") %>% # Eliminar todos los saltos de línea
    str_squish() # Eliminar los espacios en blanco antes y después del texto
  
  # Agregar un prefijo con la sección
  datos_contenedor_1 <- str_c("S1_", datos_contenedor_1)
  
  
  
  # Buscar la URL origen del mapa de Google
  datos_contenedor_mapa <-
    html_ficha %>%
    html_element(xpath = '//*[@id="divSeccion2"]') %>% # Buscar la sección donde está el mapa
    html_element("a") %>% # Buscar el primer link con la URL del mapa
    html_attr("href") %>%  # Extraer la URL del mapa
    replace_na("") # Si no hay mapa, reemplazar el valor vacío NA con un espacio
  
  # Agregar el nombre del campo al valor:
  datos_contenedor_mapa <-
    str_c("S2_Google Maps: ",
          datos_contenedor_mapa)
  
  
  # Buscar el siguiente contenedor y extraer los datos
  datos_contenedor_2 <-
    html_ficha %>% 
    html_element(xpath = '//*[@id="divSeccion2"]') %>% # Buscar el contenedor con los datos
    html_children() %>% 
    html_children() %>% # Bajar al segundo nivel (filas)
    html_text() %>% # Extraer el texto de los elementos
    str_remove_all("\n") %>% # Eliminar todos los saltos de línea
    str_squish() # Eliminar los espacios en blanco antes y después del texto
  
  # Eliminar las dos primeras filas que contienen textos de los controles del mapa
  datos_contenedor_2 <-
    datos_contenedor_2[c(-1, -2)]
  
  # Eliminar la fila de título de "Otra localización:"
  datos_contenedor_2 <-
    datos_contenedor_2[datos_contenedor_2 != "Otra localización:"]
  
  # Agregar el nombre de la variable al dato de "Otra localización:"
  datos_contenedor_2[length(datos_contenedor_2)] <-
    str_c("Otra localización: ", datos_contenedor_2[length(datos_contenedor_2)])
  
  # Agregar un prefijo con la sección
  datos_contenedor_2 <-
    str_c("S2_", datos_contenedor_2)
  
  
  # Buscar el siguiente contenedor y extraer los datos
  datos_contenedor_3_original <-
    html_ficha %>% 
    html_element(xpath = '//*[@id="divSeccion3"]') %>% # Buscar el siguiente contenedor
    html_children() %>% 
    html_children() %>% # Bajar al segundo nivel (filas)
    html_text() %>%  # Extraer el texto de cada elemento que se encontró
    str_remove_all("\n") %>% # Eliminar todos los saltos de línea
    str_squish() # Eliminar los espacios en blanco antes y después del texto
  
  # Separar los datos que vienen juntos en una misma fila
  datos_contenedor_3_separados <-
    datos_contenedor_3_original[10] %>% 
    str_replace("Género:", "|Género:") %>% # Agregar un separador
    str_replace("Tipo Arquitectónico:", "|Tipo Arquitectónico:") %>% # Agregar un separador
    str_split("\\|") %>% # Separar usando el caracter definido
    unlist() # Convertir la lista resultante en un vector de tipo caracter
  
  # Eliminar las filas de encabezados, agregar los datos separados y conservar los datos con nombre
  datos_contenedor_3 <- c(
    datos_contenedor_3_original[c(1, 3:5, 7:8)],
    datos_contenedor_3_separados,
    datos_contenedor_3_original[12]
  )
  
  # Agregar un prefijo para diferenciar los datos originales de los actuales
  datos_contenedor_3[2:4] <-
    str_c("original_", datos_contenedor_3[2:4])
  
  datos_contenedor_3[7:9] <-
    str_c("actual_", datos_contenedor_3[7:9])
  
  # Agregar un prefijo con la sección
  datos_contenedor_3 <-
    str_c("S3_", datos_contenedor_3)
  
  
  # Buscar el siguiente contenedor y extraer los datos
  datos_contenedor_4_original <-
    html_ficha %>% 
    html_element(xpath = '//*[@id="divSeccion4"]') %>% # Buscar el siguiente contenedor
    html_children() %>%
    html_children() %>%
    html_children() %>% # Bajar al tercer nivel (filas)
    html_text() %>% # Extraer el texto de cada elemento que se encontró
    str_remove_all("\n") %>% # Eliminar todos los saltos de línea
    str_squish() # Eliminar los espacios en blanco antes y después del texto
  
  # Separar los datos que vienen juntos en una misma fila
  datos_contenedor_4_separados <-
    datos_contenedor_4_original[5] %>% 
    str_replace("Ubicado en la Zona de Monumentos Históricos:", "|Ubicado en la Zona de Monumentos Históricos:") %>% 
    str_replace("Sitio Inscrito en la lista de patrimonio Mundial UNESCO:", "|Sitio Inscrito en la lista de patrimonio Mundial UNESCO:") %>% 
    str_split("\\|") %>% 
    unlist()
  
  # Unir los datos extraídos correctamente y los que se separaron
  datos_contenedor_4 <-
    c(
      datos_contenedor_4_original[1:4],
      datos_contenedor_4_separados
    )
  
  # Agregar un prefijo con la sección
  datos_contenedor_4 <-
    str_c("S4_", datos_contenedor_4)
  
  
  # Buscar el siguiente contenedor y extraer los datos
  datos_contenedor_5 <-
    html_ficha %>% 
    html_element(xpath = '//*[@id="divSeccion6"]') %>%  # Buscar el siguiente contenedor
    html_children() %>% 
    html_children() %>% 
    html_children() %>% # Bajar al tercer nivel (filas)
    html_text() %>%  # Extraer el texto de cada elemento que se encontró
    str_remove_all("\n") %>% # Eliminar todos los saltos de línea
    str_squish() # Eliminar los espacios en blanco antes y después del texto
  
  # Agregar los nombres de las variables a los datos:
  datos_contenedor_5[5] <- 
    str_c(datos_contenedor_5[4], " ", datos_contenedor_5[5])
  
  datos_contenedor_5[8] <- 
    str_c(datos_contenedor_5[7], " ", datos_contenedor_5[8])
  
  datos_contenedor_5[11] <- 
    str_c(datos_contenedor_5[10], " ", datos_contenedor_5[11])
  
  # Quitar las filas de encabezados 
  datos_contenedor_5 <-
    datos_contenedor_5[c(-1, -4, -6, -7, -10)]
  
  # Extraer el texto de cada elemento que se encontró
  datos_contenedor_5 <-
    str_c("S5_", datos_contenedor_5)
  
  
  # Buscar el siguiente contenedor y extraer los datos
  datos_contenedor_6 <-
    html_ficha %>% 
    html_element(xpath = '//*[@id="divSeccion7"]') %>% # Buscar el siguiente contenedor
    html_children() %>% # Bajar al primer nivel (filas)
    html_text() %>% # Extraer el texto de cada elemento que se encontró
    str_remove_all("\n") %>% # Eliminar todos los saltos de línea
    str_squish() # Eliminar los espacios en blanco antes y después del texto
  
  # Quitar filas vacías
  datos_contenedor_6 <-
    datos_contenedor_6[datos_contenedor_6 != ""]
  
  # Agregar un prefijo con la sección
  datos_contenedor_6 <-
    str_c("S6_", datos_contenedor_6)
  
  
  # Buscar el siguiente contenedor y extraer los datos
  datos_contenedor_7 <-
    html_ficha %>% 
    html_element(xpath = '//*[@id="divSeccion8"]') %>% # Buscar el siguiente contenedor
    html_children() %>% # Bajar al primer nivel (filas)
    html_text() %>% # Extraer el texto de cada elemento que se encontró
    str_remove_all("\n") %>% # Eliminar todos los saltos de línea
    str_squish() # Eliminar los espacios en blanco antes y después del texto
  
  # Quitar filas vacías
  datos_contenedor_7 <-
    datos_contenedor_7[datos_contenedor_7 != ""]
  
  # Agregar un prefijo con la sección y el nombre de la variable
  datos_contenedor_7 <-
    str_c("S7_Monografia: ", datos_contenedor_7)
  
  
  # Buscar el siguiente contenedor y extraer los datos
  datos_contenedor_8 <-
    html_ficha %>% 
    html_element(xpath = '//*[@id="divSeccion11"]') %>% # Buscar el siguiente contenedor
    html_children() %>% # Bajar al primer nivel (filas)
    html_text() %>% # Extraer el texto de cada elemento que se encontró
    str_remove_all("\n") %>% # Eliminar todos los saltos de línea
    str_squish() # Eliminar los espacios en blanco antes y después del texto
  
  # Quitar filas vacías
  datos_contenedor_8 <-
    datos_contenedor_8[datos_contenedor_8 != ""]
  
  datos_contenedor_8 <-
    str_c("S8_", datos_contenedor_8)
  
  
  # Buscar el siguiente contenedor y extraer los datos
  datos_contenedor_9 <-
    html_ficha %>% 
    html_element(xpath = '//*[@id="divSeccion12"]') %>% # Buscar el siguiente contenedor
    html_children() %>% 
    html_children() %>% 
    html_children() %>% # Bajar al tercer nivel (filas)
    html_text() %>% # Extraer el texto de cada elemento que se encontró
    str_remove_all("\n") %>% # Eliminar todos los saltos de línea
    str_squish() # Eliminar los espacios en blanco antes y después del texto
  
  # Definir un prefijo inicial
  prefijo <- ""
  
  # Agregar el prefijo de encabezado correspondiente a cada elemento extraido
  for(i in 1:length(datos_contenedor_9)) {
    
    if(!str_detect(datos_contenedor_9[i], "^*:")) {
      prefijo <- datos_contenedor_9[i]
    }
    
    datos_contenedor_9[i] <- str_c(str_trunc(prefijo, 6, ellipsis = ""), # Usar las primeras letras del encabezado
                                   "_", 
                                   datos_contenedor_9[i])
    
  }
  
  # Agregar un prefijo con la sección
  datos_contenedor_9 <- str_c("S9_", datos_contenedor_9) 
  
  # Quitar las cabeceras (elementos sin ":")
  datos_contenedor_9 <- 
    datos_contenedor_9[str_detect(datos_contenedor_9, "^*:")]
  
  
  # Buscar el siguiente contenedor y extraer los datos
  datos_contenedor_10 <-
    html_ficha %>% 
    html_element(xpath = '//*[@id="divSeccion17"]') %>% # Buscar el siguiente contenedor
    html_children() %>% 
    html_text() %>% # Extraer el texto de cada elemento que se encontró
    str_remove_all("\n") %>% # Eliminar todos los saltos de línea
    str_squish() # Eliminar los espacios en blanco antes y después del texto
  
  # Agregar el prefijo y conservar la fecha más reciente:
  datos_contenedor_10 <- 
    str_c("S10_", datos_contenedor_10) %>% 
    head()
  
  
  # Buscar el siguiente contenedor y extraer los datos
  datos_contenedor_11 <-
    html_ficha %>% 
    html_element(xpath = '//*[@id="divSeccion22"]') %>% # Buscar el siguiente contenedor
    html_element(xpath = '//*[@id="divSeccion22"]/div[1]/div[2]/p') %>% 
    html_text() %>% # Extraer el texto de cada elemento que se encontró
    str_remove_all("\n") %>% # Eliminar todos los saltos de línea
    str_squish() # Eliminar los espacios en blanco antes y después del texto
  
  # Agregar el nombre del campo al valor:
  datos_contenedor_11 <- c(str_c("S11_Planoteca del AHJE: ", datos_contenedor_11),
                           str_c("S11_URL Planoteca del AHJE: https://catalogonacionalmhi.inah.gob.mx/consulta_publica/generarPdfPlanoteca/", id_pagina))
  
  
  # Buscar el siguiente contenedor y extraer los datos
  datos_contenedor_12 <-
    html_ficha %>% 
    html_element(xpath = '//*[@id="divSeccion23"]') %>% # Buscar el siguiente contenedor
    html_element(xpath = '//*[@id="divSeccion23"]/div[1]/div[2]') %>% 
    html_text() %>% # Extraer el texto de cada elemento que se encontró
    str_remove_all("\n") %>% # Eliminar todos los saltos de línea
    str_squish() # Eliminar los espacios en blanco antes y después del texto
  
  # Agregar el nombre del campo al valor:
  datos_contenedor_12 <- c(str_c("S12_AHJE: ", datos_contenedor_12),
                           str_c("S12_URL AHJE: https://catalogonacionalmhi.inah.gob.mx/consulta_publica/generarPdfArchivoHistorico/", id_pagina))
  
  
  # Buscar el siguiente contenedor y extraer los datos
  datos_contenedor_13 <-
    html_ficha %>% 
    html_element(xpath = '//*[@id="divSeccion24"]') %>% # Buscar el siguiente contenedor
    html_element(xpath = '//*[@id="divSeccion24"]/div[1]/div[2]') %>% 
    html_text() %>% # Extraer el texto de cada elemento que se encontró
    str_remove_all("\n") %>% # Eliminar todos los saltos de línea
    str_squish() # Eliminar los espacios en blanco antes y después del texto
  
  # Agregar el nombre del campo al valor:
  datos_contenedor_13 <- c(str_c("S13_FCRV: ", datos_contenedor_13),
                           str_c("S13_URL FCRV: https://catalogonacionalmhi.inah.gob.mx/consulta_publica/generarPdfFototeca/", id_pagina))
  
  
  # Unir los valores extraidos
  datos_contenedor <- c(str_c("ID: ", id_pagina), # Agregar el ID de la página
                        datos_contenedor_encabezado,
                        datos_contenedor_1,
                        datos_contenedor_mapa,
                        datos_contenedor_2,
                        datos_contenedor_3,
                        datos_contenedor_4,
                        datos_contenedor_5,
                        datos_contenedor_6,
                        datos_contenedor_7,
                        datos_contenedor_8,
                        datos_contenedor_9,
                        datos_contenedor_10,
                        datos_contenedor_11,
                        datos_contenedor_12,
                        datos_contenedor_13)
  
  
  # Construir un tibble con un registro con los datos extraídos
  registro_extraido <-
    tibble(lista = datos_contenedor) %>% # Nuevo tibble con una columna con la lista
    separate(lista, into = c("variable", "valor"), sep = ":", extra = "merge") %>% # Separar en los dos puntos
    mutate(variable = make_clean_names(variable)) %>%  
    pivot_wider(names_from = variable, values_from = valor) # Voltear de filas a columnas
  
  
  # Salida en pantalla para ver el avance de la extracción de la ficha
  str_c("Extrayendo datos [", id_pagina, "] ...") %>% 
    print()
  
  # Finalmente, regresar como resultado de la función el tibble con el registro extraido
  return(registro_extraido)
  
} 
```

### Extracción masiva

```{r}
# Iniciar el servidor de Selenium
servidor_selenium <- rsDriver(browser = "firefox")

# Crear el controlador del explorador web
controlador <- servidor_selenium$client
```

```{r}
# Iterar el número de ID de la página para extraer los datos
id_inicial <- 1
id_final <- 100

datos_acumulados <- map_df(id_inicial:id_final,
                           extraer_datos,
                           controlador)

datos_acumulados %>% 
  glimpse()
```

Guardar el archivo con los datos extraídos:

```{r Guardar los datos en un archivo}
datos_acumulados %>% 
  write_excel_csv(str_c("Datos/inah_monumentos_historicos_",
                        str_pad(id_inicial, width = 5, pad = "0"),
                        "-",
                        str_pad(id_final, width = 5, pad = "0"),
                        ".csv"))
```

Cierra las ventanas abiertas del explorador y apaga el servidor de Selenium:

```{r Cerrar explorador y servidor de Selenium}
# Cerrar todas las ventanas abiertas
controlador$closeall()

# Apagar el servidor de Selenium
servidor_selenium$server$stop()

# Borra todas las variables y objetos del entorno
rm(list = ls())
```

### Geocodificación

NOTA: Por seguridad, no compartas tu llave de API, puedes copiar la instrucción y ejecutarla en la consola para no guardarla en este cuaderno.

```{r}
# Establece tu llave API de Here en cada nueva sesión de trabajo.
Sys.setenv("HERE_API_KEY" = "TU_API_KEY_AQUI")
```

```{r}
# Leer los datos extraídos:
datos_acumulados <-
  read_csv("Datos/inah_monumentos_historicos_00001-00100.csv")

# Concatenar textos para obtener la dirección en una sola variable y geocodificar:
datos_geocodificados <-
  datos_acumulados %>% 
  mutate_at(c("s2_nombre_de_la_vialidad",
              "s2_numero_exterior",
              "s2_tipo_y_nombre_del_asentamiento_humano",
              "s2_localidad_colonia",
              "s2_municipio_alcaldia",
              "s2_entidad_federativa"),
            str_replace_na,
            "") %>% 
  mutate(direccion = str_c(s2_nombre_de_la_vialidad,
                           " ",
                           s2_numero_exterior,
                           ", ",
                           s2_localidad_colonia,
                           ", ",
                           s2_municipio_alcaldia,
                           ", ",
                           s2_entidad_federativa,
                           ", México")) %>% 
  select(id, direccion) %>%
  slice_sample(n = 10) %>% 
  geocode(address = "direccion")
```

```{r}
tmap_mode("view")

datos_geocodificados %>% 
  st_as_sf(coords = c("long", "lat"),
           crs = 4326,
           remove = FALSE,
           na.fail = FALSE) %>% 
  qtm()
```
